# running simulations to test MCpbt and scobi_deux
# using parallele processing to speed up simulations

#install if haven't already
# devtools::install_github("delomast/fishCompTools")
# devtools::install_github("delomast/devMCpbt")

###################
## note that this uses forking through the parallel package, so it will not work on Windows systems
###################

#load libraries
library(fishCompTools, lib.loc = "../../Rlib/")
library(devMCpbt, lib.loc = "../../Rlib/")
library(parallel)

# first, load in the base scenario inputs

# relative sizes of the wild groups
gsiComp <- read.table("./inputs/baseScenario/gsiCompIn.txt", header = TRUE, stringsAsFactors = FALSE, sep = "\t")

# relative sizes of the unclipped hatchery groups
pbtComp <- read.table("./inputs/baseScenario/pbtCompIn.txt", header = TRUE, stringsAsFactors = FALSE, sep = "\t")

# gsi assignmnet of the unclipped hatchery groups
gsiOfPbt <- read.table("./inputs/baseScenario/gsiOfPbtIn.txt", header = TRUE, stringsAsFactors = FALSE, sep = "\t")
# normalize
for(i in 1:nrow(gsiOfPbt)){
	gsiOfPbt[i,2:ncol(gsiOfPbt)] <- gsiOfPbt[i,2:ncol(gsiOfPbt)] / sum(gsiOfPbt[i,2:ncol(gsiOfPbt)])
}
#move group names to rownames
rownames(gsiOfPbt) <- gsiOfPbt[,1]
gsiOfPbt <- gsiOfPbt[,2:ncol(gsiOfPbt)]

# true proportion of each strata that is wild
propWild <- read.table("./inputs/baseScenario/propWildIn.txt", header = TRUE, stringsAsFactors = FALSE, sep = "\t")

# population sizes
popSize <- read.table("./inputs/baseScenario/popSizeIn.txt", header = TRUE, stringsAsFactors = FALSE, sep = "\t")

# tag rates
tagRates <- read.table("./inputs/baseScenario/tagRatesIn.txt", header = TRUE, stringsAsFactors = FALSE, sep = "\t")


# scenarios to investigate:
## sample rate - THIS SCRIPT
## relatively similar tag rates
## highly variable tag rates
## one sub-variable, similar between wild and hatchery
## one sub-variable, distinct between wild and hatchery
## GSI composition similar between PBT groups
## GSI composition distinct between PBT groups

# sample rate
sampRate <- c(seq(.05, .10, .01), .15, .2, .25, .3, .35, .4)
# sampRate <- c(.05) #testing

#number of sims for each sample rate
numSims <- 1000
# numSims <- 50 #testing

# empty data structure to save results - point estimates and CIs
## for MCpbt
sr_mean <- matrix(nrow = numSims*length(sampRate), ncol = (nrow(gsiComp) + nrow(pbtComp)))
## didn't keep PBT names the same as generated by the function - woops!
allGroups <- c(gsub("PBTgroup", "pbtGroup", pbtComp$Group), gsiComp$Group)
colnames(sr_mean) <- allGroups
sr_upper <- sr_mean
sr_lower <- sr_mean
## for scobi_deux
srSD_mean <- sr_mean
srSD_upper <- sr_mean
srSD_lower <- sr_mean
srRec <- rep(0, nrow(sr_mean))
srMLE_mean <- sr_mean
srMLE_lower <- sr_mean
srMLE_upper <- sr_mean
convergeMLE <- rep(NA, nrow(sr_mean))

		
currentRow <- 1

#parallel options
countCores <- detectCores()

#set seed for R
# seed for MCpbt is separate and is set in function call
set.seed(7)


#####################
## defining function to calculate estimates and return estimates in a semi-convenient form

compFunc <- function(data){
	
	#unpack input
	trapData <- data[[1]]
	tags <- data[[2]]
	r <- data[[3]]
	popSize <- data[[4]]
	

	

	srRec[1] <- sr #sample rate corresponding to that row

	convergeBool <- NA
	
	#run MLE
	
	#reset tag rates to be accurate
	tags <- data[[2]]
	
	tryCatch({
		    conv <- capture.output(
		mlePointEstimates <- MLEwrapper(trapData, tags = tags, GSIcol = "GSI", PBTcol = "GenParentHatchery", 
												  strataCol = "StrataVar", adFinCol = "AdClip", AI = TRUE, optimMethod = "L-BFGS-B", variableCols = NULL, 
												  gr = params_grad, lower=10^-24, control = list(maxit=10000))
		)
		}, error = function(e) {
			print(e)
			save(trapData, tags, file = "./error.rda")
			srMLE_mean[1,] <- -9
		return(list(
			sr_mean[1,],
			sr_upper[1,],
			sr_lower[1,],
			srSD_mean[1,],
			srSD_lower[1,],
			srSD_upper[1,],
			srRec[1],
			srMLE_mean[1,],
			srMLE_lower[1,],
			srMLE_upper[1,],
			convergeBool
		))
		
		})

	
	convergeBool <- FALSE
	if(length(conv) > 0) convergeBool <- TRUE #true if FAIL to converge
	
	strataComp <- list()
	for(i in 1:length(mlePointEstimates)){
		strataComp[[i]] <- mlePointEstimates[[i]]$piTot * popSize[i,2]
	}
	#now sum up across strata
	for(g in allGroups){
		tempSum <- 0
		for(strat in strataComp){
			tempSum <- sum(tempSum, strat[g], na.rm = TRUE)
		}
		srMLE_mean[1,g] <- tempSum
	}
	
	###############################
	######### bootstrapping the MLE is WAY too slow to realistically evaluate coverage for the large scenario
	###############################
	# # the number of bootstraps we will perform
	# bootRep <- 50
	# # a data structure to save the estimates
	# #each row is an estimate, each column is a group
	# bootPiTot <- matrix(0, nrow = bootRep, ncol = length(allGroups))
	# colnames(bootPiTot) <- allGroups #pulling the group names from strataComp
	# #separate data by strata
	# dataByStrata <- list()
	# for(s in unique(trapData$StrataVar)){
	# 	dataByStrata[[as.character(s)]] <-  trapData[trapData$StrataVar == s,]
	# }
	# for(b in 1:bootRep){
	# 	cat("\n", b, "\n")
	# 	#resample each strata separately
	# 	bootData <- data.frame()
	# 	for(d in dataByStrata){
	# 		bootData <- rbind(bootData, d[sample(1:nrow(d), nrow(d), replace = TRUE),])
	# 	}
	# 	#get estimates for each strata with resampled data
	# 	invisible(capture.output(
	# 	bootEst <- MLEwrapper(bootData, tags = tags, GSIcol = "GSI", PBTcol = "GenParentHatchery", strataCol = "StrataVar", adFinCol = "AdClip", AI = TRUE, 
	# 								 optimMethod = "Nelder-Mead", variableCols = NULL, control = list(maxit=10000))
	# 	))
	# 	#now multiply be population size
	# 	strataComp <- list()
	# 	for(i in 1:length(bootEst)){
	# 		strataComp[[i]] <- bootEst[[i]]$piTot * popSize[i,2]
	# 	}
	# 	#now sum up across strata and add to bootPiTot
	# 	for(g in colnames(bootPiTot)){
	# 		tempSum <- 0
	# 		for(strat in strataComp){
	# 			# removign NA in case group does not exist in this resample
	# 			tempSum <- sum(tempSum, strat[g], na.rm = TRUE)
	# 		}
	# 		bootPiTot[b,g] <- tempSum
	# 	}
	# }
	# #now, let's calculate a 90% CI
	# srMLE_lower[1,allGroups] <- apply(bootPiTot[,allGroups],2,quantile, c(.05))
	# srMLE_upper[1,allGroups] <- apply(bootPiTot[,allGroups],2,quantile, c(.95))


	## utilizing the sr* data structures is a little lazy, but it works
	return(list(
		sr_mean[1,],
		sr_upper[1,],
		sr_lower[1,],
		srSD_mean[1,],
		srSD_lower[1,],
		srSD_upper[1,],
		srRec[1],
		srMLE_mean[1,],
		srMLE_lower[1,],
		srMLE_upper[1,],
		convergeBool
	))
	
}



for(sr in sampRate){
	print(sr)
	# generate data in chunks of numSims for processing
	dataList <- list()
	for(r in 1:numSims){
		trapData <- data.frame()
		for(s in 1:nrow(popSize)){
			tempData <- generatePBTGSIdata(sampRate = sr, censusSize = popSize[s,2], relSizePBTgroups = pbtComp[,(s+1)], tagRates = tagRates[,2], 
											 obsTagRates = tagRates[,2], physTagRates = rep(0,nrow(tagRates)),
					    true_clipped = 0, true_noclip_H = (1 - propWild[s,2]), true_wild = propWild[s,2], relSizeGSIgroups = gsiComp[,(s+1)], 
					    PBT_GSI_calls = gsiOfPbt, varMatList = NA)
			tempData[[1]]$StrataVar <- popSize[s,1]
			tempData[[1]]$GSI <- paste0("GSIgroup", tempData[[1]]$GSI)
			trapData <- rbind(trapData, tempData[[1]])
		}
		#get tag rate in a nice format
		tags <- tempData[[2]]
		
		#save data
		dataList[[r]] <- list(trapData, tags, r, popSize) #saving r to use as prefix for SD output files
	}
	
	# #testing
	# compFunc(dataList[[1]])
	
	#now run in parallel
	results <- mclapply(dataList, compFunc, mc.cores = countCores)
	
	#upack results and assign to sr* matrices
	for(i in 1:length(results)){
		tempRes <- results[[i]]
		if(length(tempRes) > 10){
		sr_mean[currentRow,] <- tempRes[[1]]
		sr_upper[currentRow,] <- tempRes[[2]]
		sr_lower[currentRow,] <- tempRes[[3]]
		srSD_mean[currentRow,] <- tempRes[[4]]
		srSD_lower[currentRow,] <- tempRes[[5]]
		srSD_upper[currentRow,] <- tempRes[[6]]
		srRec[currentRow] <- tempRes[[7]]
		srMLE_mean[currentRow,] <- tempRes[[8]]
		srMLE_lower[currentRow,] <- tempRes[[9]]
		srMLE_upper[currentRow,] <- tempRes[[10]]
		convergeMLE[currentRow] <- tempRes[[11]]
		} else {
			print(paste("error", currentRow))
		}
				
		currentRow <- currentRow + 1
	}
}

#calculate true values
trueComp <- rep(0, ncol(sr_mean))
names(trueComp) <- colnames(sr_mean)
for(i in 2:ncol(gsiComp)){
	tot <- popSize[(i-1),2] * propWild[(i-1),2]
	tempnorm <- gsiComp[,i] / sum(gsiComp[,i])
	trueComp[gsiComp[,1]] <- trueComp[gsiComp[,1]] + (tempnorm * tot)
}
for(i in 2:ncol(pbtComp)){
	tot <- popSize[(i-1),2] * (1-propWild[(i-1),2])
	tempnorm <- pbtComp[,i] / sum(pbtComp[,i])
	tempNames <- gsub("PBTgroup", "pbtGroup", pbtComp$Group)
	trueComp[tempNames] <- trueComp[tempNames] + (tempnorm * tot)
}

#save estimates and true values
save(sr_mean, sr_upper, sr_lower, srSD_mean, srSD_upper, srSD_lower, srRec, trueComp, convergeMLE, srMLE_mean,
	  srMLE_lower, srMLE_upper, file = "./rdaOutputs/large_MLEonly1000.rda")

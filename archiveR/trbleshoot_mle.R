# running simulations to test MCpbt and scobi_deux
# using parallele processing to speed up simulations
# this is sample rate of SMALL scenario with prior on piTot of 1/number of groups
#		and prior on piGSI as 1/N

#install if haven't already
# devtools::install_github("delomast/fishCompTools")
# devtools::install_github("delomast/devMCpbt")

###################
## note that this uses forking through the parallel package, so it will not work on Windows systems
###################

#load libraries
library(fishCompTools, lib.loc = "../../Rlib/")
library(devMCpbt, lib.loc = "../../Rlib/")
library(parallel)

# first, load in the base scenario inputs

# relative sizes of the wild groups
gsiComp <- read.table("./inputs/smallScenario/gsiCompIn.txt", header = TRUE, stringsAsFactors = FALSE, sep = "\t")

# relative sizes of the unclipped hatchery groups
pbtComp <- read.table("./inputs/smallScenario/pbtCompIn.txt", header = TRUE, stringsAsFactors = FALSE, sep = "\t")

# gsi assignmnet of the unclipped hatchery groups
gsiOfPbt <- read.table("./inputs/smallScenario/gsiOfPbtIn.txt", header = TRUE, stringsAsFactors = FALSE, sep = "\t")
# normalize
for(i in 1:nrow(gsiOfPbt)){
	gsiOfPbt[i,2:ncol(gsiOfPbt)] <- gsiOfPbt[i,2:ncol(gsiOfPbt)] / sum(gsiOfPbt[i,2:ncol(gsiOfPbt)])
}
#move group names to rownames
rownames(gsiOfPbt) <- gsiOfPbt[,1]
gsiOfPbt <- gsiOfPbt[,2:ncol(gsiOfPbt)]

# true proportion of each strata that is wild
propWild <- read.table("./inputs/smallScenario/propWildIn.txt", header = TRUE, stringsAsFactors = FALSE, sep = "\t")

# population sizes
popSize <- read.table("./inputs/smallScenario/popSizeIn.txt", header = TRUE, stringsAsFactors = FALSE, sep = "\t")

# tag rates
tagRates <- read.table("./inputs/smallScenario/tagRatesIn.txt", header = TRUE, stringsAsFactors = FALSE, sep = "\t")


# scenarios to investigate:
## sample rate - THIS SCRIPT
## relatively similar tag rates
## highly variable tag rates
## one sub-variable, similar between wild and hatchery
## one sub-variable, distinct between wild and hatchery
## GSI composition similar between PBT groups
## GSI composition distinct between PBT groups

# sample rate
sampRate <- c(seq(.05, .10, .01), .15, .2, .25, .3, .35, .4)
# sampRate <- c(.05) #testing

#number of sims for each sample rate
numSims <- 500
# numSims <- 50 #testing

# empty data structure to save results - point estimates and CIs
## for MCpbt
sr_mean <- matrix(nrow = numSims*length(sampRate), ncol = (nrow(gsiComp) + nrow(pbtComp)))
## didn't keep PBT names the same as generated by the function - woops!
allGroups <- c(gsub("PBTgroup", "pbtGroup", pbtComp$Group), gsiComp$Group)
colnames(sr_mean) <- allGroups
sr_upper <- sr_mean
sr_lower <- sr_mean
## for scobi_deux
srSD_mean <- sr_mean
srSD_upper <- sr_mean
srSD_lower <- sr_mean
srRec <- rep(0, nrow(sr_mean))
srMLE_mean <- sr_mean
srMLE_lower <- sr_mean
srMLE_upper <- sr_mean
convergeMLE <- rep(NA, nrow(sr_mean))

		
currentRow <- 1

#parallel options
countCores <- detectCores()
# countCores <- 1

#set seed for R
# seed for MCpbt is separate and is set in function call
set.seed(7)


#####################
## defining function to calculate estimates and return estimates in a semi-convenient form

compFunc <- function(data){
# data = dataList[[1]]
	#unpack input
	trapData <- data[[1]]
	tags <- data[[2]]
	r <- data[[3]]
	popSize <- data[[4]]
	
	srRec[1] <- sr #sample rate corresponding to that row


	
# trapData[trapData$GenParentHatchery == "Unassigned", "GSI"] <- "GSIgroup1"
	convergeBool <- NA
	
	#run MLE
	conv <- capture.output(
		mlePointEstimates <- MLEwrapper(trapData, tags = tags, GSIcol = "GSI", PBTcol = "GenParentHatchery", 
												  strataCol = "StrataVar", adFinCol = "AdClip", AI = TRUE, optimMethod = "Nelder-Mead", variableCols = NULL, 
												  control = list(maxit=10000))
		)
	convergeBool <- FALSE
	if(length(conv) > 0) convergeBool <- TRUE #true if FAIL to converge
	
	strataComp <- list()
	for(i in 1:length(mlePointEstimates)){
		strataComp[[i]] <- mlePointEstimates[[i]]$piTot * popSize[i,2]
	}
	#now sum up across strata
	for(g in allGroups){
		tempSum <- 0
		for(strat in strataComp){
			tempSum <- sum(tempSum, strat[g], na.rm = TRUE)
		}
		srMLE_mean[1,g] <- tempSum
	}
	
	MLEwrapper(trapData[trapData$StrataVar == 18,], tags = tags, GSIcol = "GSI", PBTcol = "GenParentHatchery", 
												  strataCol = "StrataVar", adFinCol = "AdClip", AI = TRUE, optimMethod = "Nelder-Mead", variableCols = NULL, 
												  control = list(maxit=10000))[[1]]

	
	
	
	MLEwrapper(trapData[trapData$StrataVar == 18,], tags, "GSI", "GenParentHatchery", "StrataVar", adFinCol = "AdClip", AI = TRUE, 
			  optimMethod = "BFGS", variableCols = c(), gr = params_grad, control = list(maxit = 10000))[[1]]

MLEwrapper(trapData[trapData$StrataVar == 18,], tags, "GSI", "GenParentHatchery", "StrataVar", adFinCol = "AdClip", AI = TRUE, 
			  optimMethod = "L-BFGS-B", variableCols = c(), gr = params_grad, lower=10^-24, control = list(maxit = 10000, trace=1))[[1]]

MLEwrapper(trapData[trapData$StrataVar == 18,], tags, "GSI", "GenParentHatchery", "StrataVar", adFinCol = "AdClip", AI = TRUE, 
			  optimMethod = "BFGS", variableCols = c(), control = list(maxit = 10000))[[1]]
	
	## utilizing the sr* data structures is a little lazy, but it works
	return(list(
		sr_mean[1,],
		sr_upper[1,],
		sr_lower[1,],
		srSD_mean[1,],
		srSD_lower[1,],
		srSD_upper[1,],
		srRec[1],
		srMLE_mean[1,],
		srMLE_lower[1,],
		srMLE_upper[1,],
		convergeBool
	))
	
}

# tagRates[,2] <- tagRates[,2]/3
# tagRates[1:2,2] <- tagRates[2:1,2]
# tagRates[1,2] <- .9
# tagRates[2,2] <- .1
# tagRates[,2] <- 1

for(sr in sampRate){
	print(sr)
	# generate data in chunks of numSims for processing
	dataList <- list()
	for(r in 1:numSims){
		trapData <- data.frame()
		for(s in 1:nrow(popSize)){
			tempData <- generatePBTGSIdata(sampRate = sr, censusSize = popSize[s,2], relSizePBTgroups = pbtComp[,(s+1)], tagRates = tagRates[,2], 
											 obsTagRates = tagRates[,2], physTagRates = rep(0,nrow(tagRates)),
					    true_clipped = 0, true_noclip_H = (1 - propWild[s,2]), true_wild = propWild[s,2], relSizeGSIgroups = gsiComp[,(s+1)], 
					    PBT_GSI_calls = gsiOfPbt, varMatList = NA)
			tempData[[1]]$StrataVar <- popSize[s,1]
			tempData[[1]]$GSI <- paste0("GSIgroup", tempData[[1]]$GSI)
			trapData <- rbind(trapData, tempData[[1]])
		}
		#get tag rate in a nice format
		tags <- tempData[[2]]
		
		#save data
		dataList[[r]] <- list(trapData, tags, r, popSize) #saving r to use as prefix for SD output files
	}
	
	# #testing
	# print(compFunc(dataList[[33]]))
	# break
	
	#now run in parallel
	results <- mclapply(dataList, compFunc, mc.cores = countCores)
	
	#upack results and assign to sr* matrices
	for(i in 1:length(results)){
		tempRes <- results[[i]]
		
		sr_mean[currentRow,] <- tempRes[[1]]
		sr_upper[currentRow,] <- tempRes[[2]]
		sr_lower[currentRow,] <- tempRes[[3]]
		srSD_mean[currentRow,] <- tempRes[[4]]
		srSD_lower[currentRow,] <- tempRes[[5]]
		srSD_upper[currentRow,] <- tempRes[[6]]
		srRec[currentRow] <- tempRes[[7]]
		srMLE_mean[currentRow,] <- tempRes[[8]]
		srMLE_lower[currentRow,] <- tempRes[[9]]
		srMLE_upper[currentRow,] <- tempRes[[10]]
		convergeMLE[currentRow] <- tempRes[[11]]
				
		currentRow <- currentRow + 1
	}
}

#calculate true values
trueComp <- rep(0, ncol(sr_mean))
names(trueComp) <- colnames(sr_mean)
for(i in 2:ncol(gsiComp)){
	tot <- popSize[(i-1),2] * propWild[(i-1),2]
	tempnorm <- gsiComp[,i] / sum(gsiComp[,i])
	trueComp[gsiComp[,1]] <- trueComp[gsiComp[,1]] + (tempnorm * tot)
}
for(i in 2:ncol(pbtComp)){
	tot <- popSize[(i-1),2] * (1-propWild[(i-1),2])
	tempnorm <- pbtComp[,i] / sum(pbtComp[,i])
	tempNames <- gsub("PBTgroup", "pbtGroup", pbtComp$Group)
	trueComp[tempNames] <- trueComp[tempNames] + (tempnorm * tot)
}

apply(srMLE_mean,2,mean)
trueComp
## why is 1 so poorly estimated??? is it b/c it is the normalized against parameter? so other sizes are directly optimized, but it isn't?
## try feeding a parameter for it into the estimator as well? or try the log(ratios) approach?
###
## was likely b/c it didn't have a parameter for group 1 (set to 1). changed functions (in devMCpbt) to use a parameter for all groups
## this means there are infinite number of equal optima, as parameters are scaled against each other
## shouldn't be a big problem for optim, though, b/c it just finds a local optima close to the starting values
## also makes programming the functions much simpler!


apply(srMLE_mean,1,sum)

dataList[[1]]

hist(srMLE_mean[,2], breaks = 20)
abline(v=mean(srMLE_mean[,2]))
abline(v=median(srMLE_mean[,2]))
abline(v=514)

#save estimates and true values
save(sr_mean, sr_upper, sr_lower, srSD_mean, srSD_upper, srSD_lower, srRec, trueComp, convergeMLE, srMLE_mean,
	  srMLE_lower, srMLE_upper, file = "./rdaOutputs/small_sampRate_1divN.rda")
